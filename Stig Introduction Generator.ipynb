{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "1z3_Pea631iHJCU_17feZVXbXtLT_3dvD",
      "authorship_tag": "ABX9TyM5r6RJYNJwM1Ydg47oS2Lg"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOsQe4sipDZX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTTFkaeNpWCr"
      },
      "source": [
        "stig_introductions = []\n",
        "with open('drive/MyDrive/LinkedIn_Articles/Stig Introductions/stig-introductions.txt', 'r') as f:\n",
        "  lines = f.readlines()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-_ra03IqIJk",
        "outputId": "23eb9316-700f-4730-e454-076154dff866"
      },
      "source": [
        "lines[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Some say he can play guitar with the clutch...and his carbon fibre beard is chizelled in the most streamlined way...\\n',\n",
              " 'Some say that all his pot plants are called steve.. and that he has a life size tattoo of his face.. on his face..\\n',\n",
              " 'Some say he’s actually dead... But the Grim reaper, is too scared to tell him...\\n',\n",
              " 'Some say He’s contracted every STD known to man, and that he has inflatable breasts to get him out of speeding tickets. All we know.. is\\n',\n",
              " 'Some say he nighted the queen....and that he saved the Queen from God....\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHqJghA9quqI"
      },
      "source": [
        "def clean(line):\n",
        "  # Special characters\n",
        "  line = re.sub(r\"\\n\", \"\", line)\n",
        "  # Contractions\n",
        "  line = re.sub(r\"he's\", \"he is\", line)\n",
        "  line = re.sub(r\"there's\", \"there is\", line)\n",
        "  line = re.sub(r\"We're\", \"We are\", line)\n",
        "  line = re.sub(r\"That's\", \"That is\", line)\n",
        "  line = re.sub(r\"won't\", \"will not\", line)\n",
        "  line = re.sub(r\"they're\", \"they are\", line)\n",
        "  line = re.sub(r\"Can't\", \"Cannot\", line)\n",
        "  line = re.sub(r\"wasn't\", \"was not\", line)\n",
        "  line = re.sub(r\"aren't\", \"are not\", line)\n",
        "  line = re.sub(r\"isn't\", \"is not\", line)\n",
        "  line = re.sub(r\"What's\", \"What is\", line)\n",
        "  line = re.sub(r\"haven't\", \"have not\", line)\n",
        "  line = re.sub(r\"hasn't\", \"has not\", line)\n",
        "  line = re.sub(r\"There's\", \"There is\", line)\n",
        "  line = re.sub(r\"He's\", \"He is\", line)\n",
        "  line = re.sub(r\"It's\", \"It is\", line)\n",
        "  line = re.sub(r\"You're\", \"You are\", line)\n",
        "  line = re.sub(r\"I'M\", \"I am\", line)\n",
        "  line = re.sub(r\"shouldn't\", \"should not\", line)\n",
        "  line = re.sub(r\"wouldn't\", \"would not\", line)\n",
        "  line = re.sub(r\"i'm\", \"I am\", line)\n",
        "  line = re.sub(r\"I'm\", \"I am\", line)\n",
        "  line = re.sub(r\"Isn't\", \"is not\", line)\n",
        "  line = re.sub(r\"Here's\", \"Here is\", line)\n",
        "  line = re.sub(r\"you've\", \"you have\", line)\n",
        "  line = re.sub(r\"we're\", \"we are\", line)\n",
        "  line = re.sub(r\"what's\", \"what is\", line)\n",
        "  line = re.sub(r\"couldn't\", \"could not\", line)\n",
        "  line = re.sub(r\"we've\", \"we have\", line)\n",
        "  line = re.sub(r\"who's\", \"who is\", line)\n",
        "  line = re.sub(r\"y'all\", \"you all\", line)\n",
        "  line = re.sub(r\"would've\", \"would have\", line)\n",
        "  line = re.sub(r\"it'll\", \"it will\", line)\n",
        "  line = re.sub(r\"we'll\", \"we will\", line)\n",
        "  line = re.sub(r\"We've\", \"We have\", line)\n",
        "  line = re.sub(r\"he'll\", \"he will\", line)\n",
        "  line = re.sub(r\"Y'all\", \"You all\", line)\n",
        "  line = re.sub(r\"Weren't\", \"Were not\", line)\n",
        "  line = re.sub(r\"Didn't\", \"Did not\", line)\n",
        "  line = re.sub(r\"they'll\", \"they will\", line)\n",
        "  line = re.sub(r\"they'd\", \"they would\", line)\n",
        "  line = re.sub(r\"DON'T\", \"DO NOT\", line)\n",
        "  line = re.sub(r\"they've\", \"they have\", line)\n",
        "  line = re.sub(r\"i'd\", \"I would\", line)\n",
        "  line = re.sub(r\"should've\", \"should have\", line)\n",
        "  line = re.sub(r\"where's\", \"where is\", line)\n",
        "  line = re.sub(r\"we'd\", \"we would\", line)\n",
        "  line = re.sub(r\"i'll\", \"I will\", line)\n",
        "  line = re.sub(r\"weren't\", \"were not\", line)\n",
        "  line = re.sub(r\"They're\", \"They are\", line)\n",
        "  line = re.sub(r\"let's\", \"let us\", line)\n",
        "  line = re.sub(r\"it's\", \"it is\", line)\n",
        "  line = re.sub(r\"can't\", \"cannot\", line)\n",
        "  line = re.sub(r\"don't\", \"do not\", line)\n",
        "  line = re.sub(r\"you're\", \"you are\", line)\n",
        "  line = re.sub(r\"i've\", \"I have\", line)\n",
        "  line = re.sub(r\"that's\", \"that is\", line)\n",
        "  line = re.sub(r\"i'll\", \"I will\", line)\n",
        "  line = re.sub(r\"doesn't\", \"does not\", line)\n",
        "  line = re.sub(r\"i'd\", \"I would\", line)\n",
        "  line = re.sub(r\"didn't\", \"did not\", line)\n",
        "  line = re.sub(r\"ain't\", \"am not\", line)\n",
        "  line = re.sub(r\"you'll\", \"you will\", line)\n",
        "  line = re.sub(r\"I've\", \"I have\", line)\n",
        "  line = re.sub(r\"Don't\", \"do not\", line)\n",
        "  line = re.sub(r\"I'll\", \"I will\", line)\n",
        "  line = re.sub(r\"I'd\", \"I would\", line)\n",
        "  line = re.sub(r\"Let's\", \"Let us\", line)\n",
        "  line = re.sub(r\"you'd\", \"You would\", line)\n",
        "  line = re.sub(r\"It's\", \"It is\", line)\n",
        "  line = re.sub(r\"Ain't\", \"am not\", line)\n",
        "  line = re.sub(r\"Haven't\", \"Have not\", line)\n",
        "  line = re.sub(r\"Could've\", \"Could have\", line)\n",
        "  line = re.sub(r\"youve\", \"you have\", line)\n",
        "  line = re.sub(r\"doesn’t\", \"does not\", line)\n",
        "  line = re.sub(r\"aren’t\", \"are not\", line)\n",
        "  line = re.sub(r\"haven’t\", \"have not\", line)\n",
        "  line = re.sub(r\"he’d\", \"he would\", line)\n",
        "  line = re.sub(r\"he’ll\", \"he will\", line)\n",
        "  line = re.sub(r\"he’s\", \"he is\", line)\n",
        "  line = re.sub(r\"isn’t\", \"is not\", line)\n",
        "  line = re.sub(r\"it’s\", \"it is\", line)\n",
        "  line = re.sub(r\"i’m\", \"i am\", line)\n",
        "  line = re.sub(r\"i’ve\", \"i have\", line)\n",
        "  line = re.sub(r\"that’s\", \"that is\", line)\n",
        "  line = re.sub(r\"there’s\", \"there is\", line)\n",
        "  line = re.sub(r\"they’d\", \"they would\", line)\n",
        "  line = re.sub(r\"we’d\", \"we would\", line)\n",
        "  line = re.sub(r\"we’ve\", \"we have\", line)\n",
        "  line = re.sub(r\"wouldn’t\", \"would not\", line)\n",
        "  line = re.sub(r\"would’ve\", \"would have\", line)\n",
        "  line = re.sub(r\"you’d\", \"you would\", line)\n",
        "  line = re.sub(r\"you’ll\", \"you will\", line)\n",
        "\n",
        "  # Specific ones\n",
        "  line = re.sub(r\"dolphin’s\", \"dolphin\", line)\n",
        "  line = re.sub(r\"jesus”\", \"jesus\", line)\n",
        "  line = re.sub(r\"man’s\", \"mans\", line)\n",
        "  line = re.sub(r\"old’s\", \"olds\", line)\n",
        "  line = re.sub(r\"spears’\", \"spears\", line)\n",
        "  line = re.sub(r\"stig’s\", \"stig\", line)\n",
        "  line = re.sub(r\"week’s\", \"week\", line)\n",
        "  line = re.sub(r\"woman’s\", \"womans\", line)\n",
        "  line = re.sub(r\"world’s\", \"worlds\", line)\n",
        "  line = re.sub(r\"“the\", \"the\", line)\n",
        "  line = re.sub(r\"“the”\", \"the\", line)\n",
        "  line = re.sub(r\"“stigflu”\", \"stig flu\", line)\n",
        "\n",
        "  # Words with punctuations and special characters\n",
        "  punctuations = '@#!?+&*[]-%,.:/();$=><|{}^\\'`\"'\n",
        "  for p in punctuations:\n",
        "    line = line.replace(p, f' {p} ')\n",
        "\n",
        "  # ... and ..\n",
        "  line = line.replace('...', ' ... ')\n",
        "  if '...' not in line:\n",
        "    line = line.replace('..', ' ... ')\n",
        "  \n",
        "  return line"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bE8eL-nqLPO"
      },
      "source": [
        "longest_sentence = ''\n",
        "clean_lines = []\n",
        "for line in lines:\n",
        "  clean_lines.append(clean(line.lower()))\n",
        "  longest_sentence = line if len(line) > len(longest_sentence) else longest_sentence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Et8YWVbL0BPY",
        "outputId": "d516f765-bb24-498c-a73c-543ec21a70c4"
      },
      "source": [
        "longest_sentence"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Some say that to unlock him, you have to run your finger down his face, like this (Clarkson then walks up to a fan from the audience and does a sort of 'swipe' down his face), and that if he was getting divorced from Paul McCartney, he'd keep his stupid whiny mouth shut!\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AITcIqpJsoU_",
        "outputId": "bc408f18-8c39-4814-d0da-cdadae734c06"
      },
      "source": [
        "clean_lines[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['some say he can play guitar with the clutch .  .  . and his carbon fibre beard is chizelled in the most streamlined way .  .  . ',\n",
              " 'some say that all his pot plants are called steve .  .  and that he has a life size tattoo of his face .  .  on his face .  . ',\n",
              " 'some say he is actually dead .  .  .  but the grim reaper ,  is too scared to tell him .  .  . ',\n",
              " 'some say he is contracted every std known to man ,  and that he has inflatable breasts to get him out of speeding tickets .  all we know .  .  is',\n",
              " 'some say he nighted the queen .  .  .  . and that he saved the queen from god .  .  .  . ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-2zSuSmJec7"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_lines)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpYRMwBN5R2d"
      },
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in clean_lines:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+2]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhpYFRzzKvri"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(layers.Bidirectional(layers.LSTM(64)))\n",
        "model.add(layers.Dense(total_words, activation='softmax'))\n",
        "model.compile(loss=categorical_crossentropy, optimizer=Adam(lr=0.001), metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtX5badyLb1u",
        "outputId": "36738416-5c4f-45ba-d7c8-3702b28c59fe"
      },
      "source": [
        "history = model.fit(xs, ys, epochs=200, batch_size=128, validation_split=0.05)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 35s 41ms/step - loss: 6.5604 - accuracy: 0.0230 - val_loss: 6.2793 - val_accuracy: 0.0238\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 5.5704 - accuracy: 0.0692 - val_loss: 6.5615 - val_accuracy: 0.0286\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.4249 - accuracy: 0.0613 - val_loss: 6.6989 - val_accuracy: 0.0238\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.4395 - accuracy: 0.0694 - val_loss: 6.7289 - val_accuracy: 0.0238\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.2938 - accuracy: 0.0710 - val_loss: 6.7288 - val_accuracy: 0.0238\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.1532 - accuracy: 0.0809 - val_loss: 6.7191 - val_accuracy: 0.0286\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.1331 - accuracy: 0.0761 - val_loss: 6.7427 - val_accuracy: 0.0333\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.0399 - accuracy: 0.0964 - val_loss: 6.7823 - val_accuracy: 0.0429\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.9159 - accuracy: 0.1392 - val_loss: 6.7947 - val_accuracy: 0.0524\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 4.8409 - accuracy: 0.1404 - val_loss: 6.8624 - val_accuracy: 0.0619\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 4.8205 - accuracy: 0.1456 - val_loss: 6.9096 - val_accuracy: 0.0571\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.6862 - accuracy: 0.1659 - val_loss: 6.9334 - val_accuracy: 0.0667\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.5945 - accuracy: 0.1893 - val_loss: 6.9707 - val_accuracy: 0.0762\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.5852 - accuracy: 0.1725 - val_loss: 6.9902 - val_accuracy: 0.0810\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 4.4409 - accuracy: 0.2066 - val_loss: 7.0326 - val_accuracy: 0.0810\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.3741 - accuracy: 0.1954 - val_loss: 7.0317 - val_accuracy: 0.0857\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.3030 - accuracy: 0.2173 - val_loss: 7.0814 - val_accuracy: 0.0857\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.1717 - accuracy: 0.2198 - val_loss: 7.1192 - val_accuracy: 0.0810\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.1168 - accuracy: 0.2283 - val_loss: 7.1258 - val_accuracy: 0.0905\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.0565 - accuracy: 0.2389 - val_loss: 7.1766 - val_accuracy: 0.0905\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 4.0006 - accuracy: 0.2558 - val_loss: 7.2075 - val_accuracy: 0.0905\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.9306 - accuracy: 0.2659 - val_loss: 7.2386 - val_accuracy: 0.1048\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 3.8198 - accuracy: 0.2733 - val_loss: 7.2538 - val_accuracy: 0.1048\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 3.7411 - accuracy: 0.2910 - val_loss: 7.2983 - val_accuracy: 0.1048\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.6434 - accuracy: 0.2895 - val_loss: 7.3198 - val_accuracy: 0.1048\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.6694 - accuracy: 0.2864 - val_loss: 7.3739 - val_accuracy: 0.1143\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.5019 - accuracy: 0.3114 - val_loss: 7.4028 - val_accuracy: 0.1095\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.4557 - accuracy: 0.3210 - val_loss: 7.4503 - val_accuracy: 0.0952\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.3927 - accuracy: 0.3243 - val_loss: 7.5012 - val_accuracy: 0.1048\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.2647 - accuracy: 0.3447 - val_loss: 7.5532 - val_accuracy: 0.0952\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.2267 - accuracy: 0.3412 - val_loss: 7.5896 - val_accuracy: 0.0952\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.1099 - accuracy: 0.3687 - val_loss: 7.6812 - val_accuracy: 0.1000\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.0214 - accuracy: 0.3724 - val_loss: 7.6790 - val_accuracy: 0.0905\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.9384 - accuracy: 0.4015 - val_loss: 7.7472 - val_accuracy: 0.1000\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.8520 - accuracy: 0.4175 - val_loss: 7.7506 - val_accuracy: 0.1000\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.8258 - accuracy: 0.4266 - val_loss: 7.8159 - val_accuracy: 0.0952\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.7040 - accuracy: 0.4507 - val_loss: 7.8633 - val_accuracy: 0.1000\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.6672 - accuracy: 0.4434 - val_loss: 7.8528 - val_accuracy: 0.0905\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.5357 - accuracy: 0.4856 - val_loss: 7.9224 - val_accuracy: 0.0905\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.4979 - accuracy: 0.4907 - val_loss: 7.9753 - val_accuracy: 0.0857\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.4018 - accuracy: 0.5101 - val_loss: 8.0171 - val_accuracy: 0.0952\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2.3540 - accuracy: 0.5297 - val_loss: 8.0487 - val_accuracy: 0.0905\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2.3025 - accuracy: 0.5272 - val_loss: 8.0971 - val_accuracy: 0.0810\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.2220 - accuracy: 0.5499 - val_loss: 8.1462 - val_accuracy: 0.0857\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.1393 - accuracy: 0.5639 - val_loss: 8.1694 - val_accuracy: 0.0905\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2.0690 - accuracy: 0.5894 - val_loss: 8.1938 - val_accuracy: 0.0857\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.0229 - accuracy: 0.5779 - val_loss: 8.2797 - val_accuracy: 0.0857\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.9310 - accuracy: 0.6011 - val_loss: 8.2766 - val_accuracy: 0.0857\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.8416 - accuracy: 0.6362 - val_loss: 8.3486 - val_accuracy: 0.0810\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.8278 - accuracy: 0.6387 - val_loss: 8.3698 - val_accuracy: 0.0810\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.7518 - accuracy: 0.6456 - val_loss: 8.4198 - val_accuracy: 0.0857\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.7154 - accuracy: 0.6600 - val_loss: 8.4695 - val_accuracy: 0.0857\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.7200 - accuracy: 0.6616 - val_loss: 8.4706 - val_accuracy: 0.0857\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.6510 - accuracy: 0.6693 - val_loss: 8.5868 - val_accuracy: 0.0810\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.5717 - accuracy: 0.6957 - val_loss: 8.5429 - val_accuracy: 0.0857\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.5385 - accuracy: 0.7019 - val_loss: 8.6156 - val_accuracy: 0.0952\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.4846 - accuracy: 0.7108 - val_loss: 8.6297 - val_accuracy: 0.0905\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.4434 - accuracy: 0.7182 - val_loss: 8.6605 - val_accuracy: 0.0905\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.3931 - accuracy: 0.7376 - val_loss: 8.6646 - val_accuracy: 0.0905\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 1.4009 - accuracy: 0.7263 - val_loss: 8.7022 - val_accuracy: 0.0857\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.3544 - accuracy: 0.7244 - val_loss: 8.7669 - val_accuracy: 0.0952\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.3150 - accuracy: 0.7372 - val_loss: 8.7495 - val_accuracy: 0.0857\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.2592 - accuracy: 0.7520 - val_loss: 8.7569 - val_accuracy: 0.0857\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.2280 - accuracy: 0.7494 - val_loss: 8.8183 - val_accuracy: 0.0952\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.2128 - accuracy: 0.7565 - val_loss: 8.8929 - val_accuracy: 0.0905\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.1777 - accuracy: 0.7587 - val_loss: 8.8469 - val_accuracy: 0.0952\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.1324 - accuracy: 0.7759 - val_loss: 8.8890 - val_accuracy: 0.0857\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.1100 - accuracy: 0.7882 - val_loss: 8.9217 - val_accuracy: 0.0810\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.0983 - accuracy: 0.7873 - val_loss: 8.9652 - val_accuracy: 0.0810\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.0771 - accuracy: 0.7807 - val_loss: 9.0058 - val_accuracy: 0.0810\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.0364 - accuracy: 0.7999 - val_loss: 9.0264 - val_accuracy: 0.0905\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 1.0102 - accuracy: 0.7997 - val_loss: 9.0502 - val_accuracy: 0.0857\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.9810 - accuracy: 0.8065 - val_loss: 9.0340 - val_accuracy: 0.0952\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.9485 - accuracy: 0.8096 - val_loss: 9.1069 - val_accuracy: 0.0905\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.9385 - accuracy: 0.8098 - val_loss: 9.1125 - val_accuracy: 0.0857\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.9342 - accuracy: 0.8168 - val_loss: 9.1222 - val_accuracy: 0.0952\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8837 - accuracy: 0.8172 - val_loss: 9.1994 - val_accuracy: 0.0905\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8853 - accuracy: 0.8325 - val_loss: 9.2081 - val_accuracy: 0.0857\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8778 - accuracy: 0.8229 - val_loss: 9.2430 - val_accuracy: 0.0857\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8653 - accuracy: 0.8199 - val_loss: 9.2614 - val_accuracy: 0.0810\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8529 - accuracy: 0.8301 - val_loss: 9.2889 - val_accuracy: 0.0905\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8037 - accuracy: 0.8388 - val_loss: 9.3055 - val_accuracy: 0.0905\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.8014 - accuracy: 0.8326 - val_loss: 9.3191 - val_accuracy: 0.0905\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7909 - accuracy: 0.8354 - val_loss: 9.3458 - val_accuracy: 0.0714\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7871 - accuracy: 0.8323 - val_loss: 9.3596 - val_accuracy: 0.0857\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7578 - accuracy: 0.8378 - val_loss: 9.4275 - val_accuracy: 0.0762\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7469 - accuracy: 0.8477 - val_loss: 9.4674 - val_accuracy: 0.0762\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7140 - accuracy: 0.8479 - val_loss: 9.4161 - val_accuracy: 0.0857\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7147 - accuracy: 0.8481 - val_loss: 9.5054 - val_accuracy: 0.0762\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7036 - accuracy: 0.8472 - val_loss: 9.5332 - val_accuracy: 0.0714\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.6789 - accuracy: 0.8491 - val_loss: 9.5055 - val_accuracy: 0.0952\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6807 - accuracy: 0.8641 - val_loss: 9.5233 - val_accuracy: 0.0762\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6651 - accuracy: 0.8569 - val_loss: 9.5109 - val_accuracy: 0.0762\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6878 - accuracy: 0.8440 - val_loss: 9.5795 - val_accuracy: 0.0762\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6672 - accuracy: 0.8535 - val_loss: 9.5857 - val_accuracy: 0.0714\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6194 - accuracy: 0.8689 - val_loss: 9.6371 - val_accuracy: 0.0762\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6228 - accuracy: 0.8628 - val_loss: 9.6573 - val_accuracy: 0.0762\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.6099 - accuracy: 0.8705 - val_loss: 9.7113 - val_accuracy: 0.0714\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5907 - accuracy: 0.8699 - val_loss: 9.7129 - val_accuracy: 0.0762\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5862 - accuracy: 0.8751 - val_loss: 9.7339 - val_accuracy: 0.0762\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5841 - accuracy: 0.8725 - val_loss: 9.7430 - val_accuracy: 0.0857\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5809 - accuracy: 0.8733 - val_loss: 9.7583 - val_accuracy: 0.0857\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5656 - accuracy: 0.8740 - val_loss: 9.8011 - val_accuracy: 0.0810\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5638 - accuracy: 0.8792 - val_loss: 9.8096 - val_accuracy: 0.0857\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5415 - accuracy: 0.8712 - val_loss: 9.7872 - val_accuracy: 0.0857\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5330 - accuracy: 0.8753 - val_loss: 9.8339 - val_accuracy: 0.0857\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.5411 - accuracy: 0.8786 - val_loss: 9.8597 - val_accuracy: 0.0857\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5367 - accuracy: 0.8785 - val_loss: 9.9317 - val_accuracy: 0.0810\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5173 - accuracy: 0.8807 - val_loss: 9.9274 - val_accuracy: 0.0857\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5269 - accuracy: 0.8765 - val_loss: 9.9261 - val_accuracy: 0.0952\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4971 - accuracy: 0.8884 - val_loss: 9.9407 - val_accuracy: 0.0952\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5124 - accuracy: 0.8805 - val_loss: 9.9925 - val_accuracy: 0.0810\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4882 - accuracy: 0.8845 - val_loss: 9.9944 - val_accuracy: 0.0762\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4704 - accuracy: 0.8907 - val_loss: 9.9858 - val_accuracy: 0.0905\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.5004 - accuracy: 0.8803 - val_loss: 10.0312 - val_accuracy: 0.0810\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4976 - accuracy: 0.8850 - val_loss: 10.0523 - val_accuracy: 0.0905\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.4794 - accuracy: 0.8810 - val_loss: 10.0881 - val_accuracy: 0.0810\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.4758 - accuracy: 0.8916 - val_loss: 10.0824 - val_accuracy: 0.0905\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4482 - accuracy: 0.8910 - val_loss: 10.1003 - val_accuracy: 0.0857\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4683 - accuracy: 0.8918 - val_loss: 10.1423 - val_accuracy: 0.0857\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4502 - accuracy: 0.8898 - val_loss: 10.1519 - val_accuracy: 0.0857\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.4396 - accuracy: 0.8860 - val_loss: 10.2062 - val_accuracy: 0.0762\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4428 - accuracy: 0.8983 - val_loss: 10.2030 - val_accuracy: 0.0810\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4616 - accuracy: 0.8817 - val_loss: 10.2404 - val_accuracy: 0.0857\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4514 - accuracy: 0.8940 - val_loss: 10.2286 - val_accuracy: 0.0810\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4350 - accuracy: 0.8928 - val_loss: 10.2354 - val_accuracy: 0.0810\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.4294 - accuracy: 0.8938 - val_loss: 10.2779 - val_accuracy: 0.0952\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4203 - accuracy: 0.8976 - val_loss: 10.2600 - val_accuracy: 0.0762\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3999 - accuracy: 0.9001 - val_loss: 10.2934 - val_accuracy: 0.0857\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4303 - accuracy: 0.8903 - val_loss: 10.2801 - val_accuracy: 0.0905\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4131 - accuracy: 0.8927 - val_loss: 10.2687 - val_accuracy: 0.0857\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4104 - accuracy: 0.8963 - val_loss: 10.3077 - val_accuracy: 0.0952\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.4034 - accuracy: 0.8968 - val_loss: 10.3262 - val_accuracy: 0.0905\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4121 - accuracy: 0.8920 - val_loss: 10.3482 - val_accuracy: 0.0905\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4030 - accuracy: 0.8933 - val_loss: 10.3710 - val_accuracy: 0.0905\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4106 - accuracy: 0.8949 - val_loss: 10.3830 - val_accuracy: 0.0905\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3949 - accuracy: 0.9019 - val_loss: 10.4120 - val_accuracy: 0.0952\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3855 - accuracy: 0.8947 - val_loss: 10.3962 - val_accuracy: 0.0905\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3803 - accuracy: 0.9049 - val_loss: 10.3948 - val_accuracy: 0.0857\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3792 - accuracy: 0.9010 - val_loss: 10.4665 - val_accuracy: 0.0857\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3994 - accuracy: 0.8944 - val_loss: 10.4564 - val_accuracy: 0.0905\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3765 - accuracy: 0.9017 - val_loss: 10.4890 - val_accuracy: 0.0952\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3855 - accuracy: 0.9012 - val_loss: 10.5077 - val_accuracy: 0.0810\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3703 - accuracy: 0.9046 - val_loss: 10.5324 - val_accuracy: 0.0810\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3825 - accuracy: 0.8943 - val_loss: 10.5563 - val_accuracy: 0.0762\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3773 - accuracy: 0.8983 - val_loss: 10.5418 - val_accuracy: 0.0857\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3854 - accuracy: 0.8978 - val_loss: 10.5965 - val_accuracy: 0.0952\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3418 - accuracy: 0.9057 - val_loss: 10.5717 - val_accuracy: 0.0952\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3479 - accuracy: 0.9081 - val_loss: 10.5844 - val_accuracy: 0.0905\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3292 - accuracy: 0.9140 - val_loss: 10.5840 - val_accuracy: 0.0952\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3525 - accuracy: 0.9061 - val_loss: 10.6412 - val_accuracy: 0.0857\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3528 - accuracy: 0.9037 - val_loss: 10.5947 - val_accuracy: 0.0905\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3558 - accuracy: 0.9026 - val_loss: 10.6083 - val_accuracy: 0.0952\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3372 - accuracy: 0.9068 - val_loss: 10.7000 - val_accuracy: 0.0857\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3537 - accuracy: 0.9004 - val_loss: 10.7211 - val_accuracy: 0.0905\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3564 - accuracy: 0.9000 - val_loss: 10.6764 - val_accuracy: 0.0952\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3254 - accuracy: 0.9092 - val_loss: 10.7454 - val_accuracy: 0.0905\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3316 - accuracy: 0.9160 - val_loss: 10.6322 - val_accuracy: 0.0810\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3416 - accuracy: 0.9031 - val_loss: 10.7216 - val_accuracy: 0.0810\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3256 - accuracy: 0.9139 - val_loss: 10.7033 - val_accuracy: 0.0905\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3341 - accuracy: 0.9090 - val_loss: 10.7578 - val_accuracy: 0.0905\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3245 - accuracy: 0.9070 - val_loss: 10.7619 - val_accuracy: 0.0952\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3551 - accuracy: 0.8984 - val_loss: 10.7582 - val_accuracy: 0.0857\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3052 - accuracy: 0.9186 - val_loss: 10.8026 - val_accuracy: 0.0905\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3182 - accuracy: 0.9125 - val_loss: 10.7971 - val_accuracy: 0.0905\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3212 - accuracy: 0.9057 - val_loss: 10.8308 - val_accuracy: 0.0857\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3320 - accuracy: 0.9057 - val_loss: 10.8092 - val_accuracy: 0.0905\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3215 - accuracy: 0.9073 - val_loss: 10.8600 - val_accuracy: 0.0810\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3360 - accuracy: 0.9106 - val_loss: 10.8670 - val_accuracy: 0.0810\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3155 - accuracy: 0.9074 - val_loss: 10.8773 - val_accuracy: 0.0857\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3349 - accuracy: 0.9045 - val_loss: 10.7898 - val_accuracy: 0.0905\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3266 - accuracy: 0.9050 - val_loss: 10.8721 - val_accuracy: 0.0905\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3230 - accuracy: 0.9080 - val_loss: 10.8610 - val_accuracy: 0.0905\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3064 - accuracy: 0.9125 - val_loss: 10.8890 - val_accuracy: 0.0857\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3070 - accuracy: 0.9140 - val_loss: 10.9145 - val_accuracy: 0.0905\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3286 - accuracy: 0.9079 - val_loss: 10.9515 - val_accuracy: 0.0857\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2926 - accuracy: 0.9103 - val_loss: 10.9398 - val_accuracy: 0.0810\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3056 - accuracy: 0.9106 - val_loss: 10.9271 - val_accuracy: 0.0857\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3128 - accuracy: 0.9091 - val_loss: 10.9643 - val_accuracy: 0.0857\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3059 - accuracy: 0.9112 - val_loss: 11.0598 - val_accuracy: 0.0810\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3012 - accuracy: 0.9087 - val_loss: 10.9957 - val_accuracy: 0.0810\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2942 - accuracy: 0.9085 - val_loss: 11.0505 - val_accuracy: 0.0762\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3131 - accuracy: 0.9016 - val_loss: 11.0300 - val_accuracy: 0.0810\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2994 - accuracy: 0.9055 - val_loss: 11.0542 - val_accuracy: 0.0857\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3075 - accuracy: 0.9030 - val_loss: 11.1039 - val_accuracy: 0.0762\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2826 - accuracy: 0.9145 - val_loss: 11.0943 - val_accuracy: 0.0762\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3090 - accuracy: 0.9072 - val_loss: 11.1090 - val_accuracy: 0.0810\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3001 - accuracy: 0.9074 - val_loss: 11.0830 - val_accuracy: 0.0762\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2932 - accuracy: 0.9117 - val_loss: 11.0798 - val_accuracy: 0.0762\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.3047 - accuracy: 0.9075 - val_loss: 11.1173 - val_accuracy: 0.0762\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2860 - accuracy: 0.9070 - val_loss: 11.1349 - val_accuracy: 0.0762\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2870 - accuracy: 0.9153 - val_loss: 11.1211 - val_accuracy: 0.0714\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2915 - accuracy: 0.9030 - val_loss: 11.1305 - val_accuracy: 0.0762\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2874 - accuracy: 0.9131 - val_loss: 11.1762 - val_accuracy: 0.0714\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2883 - accuracy: 0.9071 - val_loss: 11.1602 - val_accuracy: 0.0762\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2952 - accuracy: 0.9098 - val_loss: 11.1800 - val_accuracy: 0.0762\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.2894 - accuracy: 0.9058 - val_loss: 11.1630 - val_accuracy: 0.0905\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2821 - accuracy: 0.9102 - val_loss: 11.2130 - val_accuracy: 0.0810\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2839 - accuracy: 0.9071 - val_loss: 11.2298 - val_accuracy: 0.0810\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.2656 - accuracy: 0.9130 - val_loss: 11.2482 - val_accuracy: 0.0810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "7PMDiMqjLq01",
        "outputId": "04d389fb-57ad-487d-e094-38db6bdeee64"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel('Training accuracy per epochs')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gc5bn+8e+jYsmW5CJL7r2CDW4YYxJaqA6hl0MJhBASTnIOgfQfOSGEkJCEkBBIICEQeggESMGAKaGYbtww7r3KTZKLLMmWLGmf3x87MmthS2Nbq11p78917aWd2dndW6PVPDvzzryvuTsiIpK60hIdQEREEkuFQEQkxakQiIikOBUCEZEUp0IgIpLiMhId4EAVFBT4gAEDEh1DRKRVmTVrVqm7F+7rsVZXCAYMGMDMmTMTHUNEpFUxszX7e0yHhkREUpwKgYhIilMhEBFJcSoEIiIpToVARCTFqRCIiKQ4FQIRkRTX6q4jEBFJFHcn4pCeZk0uG4k4xeXVFG3byfrtuxjZqyNDuuUBsHbLTlaUVnDC0EIqqmt5avpaaiNOz07ZHNm7E2lpxvadNZRWVNOnS3uGdsujXUb8vrerEIhIUlheXE7Zrlq6d8yid+f2lFRUs3xzBeP6d2FHVQ2LN5bzmcFdSTNj1tptVNXU0TE7kwEFOXRqn8kbizfz+9eXc0Tvjpx5RE96dMqmIC+LvKwMauqcd5eXsGNXLeeN7b3nPSMR549Tl/Pgu6twICPNyEhLwwxOHFbIT88dyX8WbmbRxh1EHJ77aD3bd9Vw9IB8PjO4K4f17Ei6Gfk57cjOTGP1lkr+9uE63l9Ryq6aOhoO9zK+fxe2Vu5mZWklACcNL2RTWRWLN5U3um7aZ6bz2SEFXHPcQI4d3LW5V70KgYg0r41lu0gzo3vHbADmFZXx2AerGdGrI6P6dGbZ5nJemLuRVaWVDOuey8RBXdm8o5qH3lu15zU6d8ikbFcN7pCXncGu3XXURpwJA/KJuDNzzba93rNLh0y27ayhT5f2PD2ziL9OW7vnMTP22iA7zqSRPZkybyN/n7mO6au2cvJh3ejbpT01Eae2LkJldR1PzVjHa4uKKa2o3vMaxw0pYGBBDtNWbuGXLy3e5++fn9OOC8f1oXOHTLp1zKZPl/Z0z8vm5QWbmLqkmKHdc7lsQj/M4JcvLSYrI43HvjKBCQPzWb2lkkUbd5BmRsfsTLrmtmPNlp1MX7WVNxYXs7Vyd/P9oWJYaxuhbPz48a4uJkTip6K6luXFFdRFHHdn845qOrXPZFj3XArzsvjb9LX8ddpaunfMIisjjYrqWiqqasnKTCfN4MNVW0kz47TDu1NRXct7K0ppl55GdW1kz3v0y+/A6L6dWbxxB8uKKwC4cmJ/Tj68G+u37WJeURk9OmVzeM+OvLF4M507tKNvl/b86qXFZKSncePnD2NIt1y2Ve5mVWklq0or6d25PdeeOIhdu+uYt76M0opqSst3s6OqBgPG9OvMn6auYOGGHeRlZ7JpRxW9O7fnGycN5ovH9MNs78M9//5oPXe8soRrjhvIVZ8ZQE1dhOzM9D2PF++oYu3WnUQcSiuq2bW7jp6dshnXv8teyzVmXlEZ7dulM6RbbpPLHshhqX0xs1nuPn6fj6kQiLQta7ZUkpuVQdfcLHbXRqipi5CTFd3531i2iwfeXsWJwws5fkgB1bURyqtr2LC9io/XbWfyxxuY1eDbdqzendvvOd7tDnURJy87g5ysDHbV1FFeVctpI7pTWV3Lc3PW071jNscPLeR/PjeYjdurWLd1JwMKchhcmLNnw7th+y4qq2sZ2j2vyd+teEcVmelpdMlpd1DrZsP2XZxzz3v06dKeH0wazrGDun6qALRVKgQibYi7UxtxMtPT2Fi2iw3bqxjXrzNmxrvLSrnm0RnkZmXw1eMH8eC7q9hSWc3ArjlMHNyVVxdsprSiGoA0g0iDf/+h3XI588iejOzVcU/jZLe8bLbv2s28ojI+WLmFCQPz+foJg0k7yG+miVZTFyEjzVKmANRTIRBphbZUVFNZXUe/rh3YXRvhveWlvLG4mDcWF7OxbBe9u7SnaNsu3OH4oQX079qBZ2YWMaBrDnXuLC+uYGSvjpw2ojvzisp4f8UWenbK5g+Xj2Xp5nKWF1eQm5VJbnYGhblZjOjZkb757VNuA5kqGisEaiwWSQLbd+5mxuptjO/fhZq6CH9+eyVPfLiGSASuO3kIU+ZtZPGm8j1nj5w7phert1Ry/tg+dMzO4K7XljFrzTZOGl7ILy8YRVZGGh+s2MKJwwvJTI9+s99dGyEzPfpNeGSvTgn+jSWZqBCIxEnxjir++dF6Bhfmcurh3di8o5qFG8tYuGEHU5eUsKq0ksrdtXTNyaK4vIqaOqddRhoG1Eac88b0prSimjv/s5SC3CzuuXwspx7efZ8NkVce2580sz0bfYBTR3Tfa5l4nocurZsKgUgz2bm7lslzNvDwe6vZUlnN9p011AYH4fsEh3HqHdm7E2cc0YOcdumUVuyma047jh9WyJuLi6mLOF89fiD9u+YQiTj/WbSZowfkk99IA2lWRrizVET2RYVA5CDt3F3LfW+t5K2lJdTURlhWXE5NnXNE746cMbIHnTtkcsG4Pry9tITXFxVz2YR+TByUz+DCXDp32PdG/cRhe48kmJZmnDGyR0v8OpLCVAhEDkBVTR0vzd/I64uKmbZyC6UVu5kwIJ/8DpmcMKyQE4cVMnFQ/l4NroMLc7n6swMTmFqkcSoEIvtQXVv3qcMt7y0v5fonP2JL5W56dMzmmEFduerYAUwYmJ+glCLNQ4VAJIa786e3VvDbV5dy4bje/GDSYRTkZvH0zHX83z/nMagwhz9cNpZjB6fOhUjS9qkQiBA9w+fNJcW8tbSEKfM2MbpvZ/45ez0vzd/ECcMKeXHuRo4bUsAfrxhHx+zMRMcVaVYqBJKS3l9RyrOziphbVEZtXYQ1W3fiHu3s7BsnDeb7pw9nZWklt76wkBfnbuSS8X35+flH7HV6pkhboUIgKWdu0XaufHA6uVkZHD0gn+zMNM4f24dJR/RgWPfcPYd8hnTL5dGrj6Zo2y76dNEVt9J2qRBIm1ReVUNJeTX98juwubyaKXM38o/ZRQztnseijTsozM3ilW+dQKcOjR/mMTP65ndoodQiiaFCIG1K2a4a/u+f8/jPos3sro2QnmbUBRd1je7bmdcXbWbn7joe/cqEJouASKpQIZA2o7yqhqsems7CDTu4YmJ/DuuRx8rSSnp1zuaYgV0Z3iOP0opq1mzZyVH9uyQ6rkjSUCGQVq2qpo4PVmzh/RWl/HP2esp21fCnK47itAb97NQryM2iIDerhVOKJDcVAmm1ausiXP7ANGav3U56mnHKYd342gmDOHqALvASORAqBNJq/fntlcxeu52fnTuSC8b12TMKl4gcmLj+55jZJOBuIB34i7v/qsHj/YBHgc7BMje6+5R4ZpLWKRJxHnx3FX94YxmZ6WlkpqdRXF7FF0b15MpjByQ6nkirFrdCYGbpwL3AaUARMMPMJrv7wpjFbgKedvc/mdkIYAowIF6ZpHVyd65/6iNemLuRE4YV0rtze+oiEbrmZvH1EwYnOp5IqxfPPYIJwHJ3XwlgZk8B5wKxhcCBjsH9TsCGOOaRVuqh91bzwtyNfPe0YVx38hBd2CXSzOJZCHoD62Kmi4BjGixzC/CqmX0TyAFO3dcLmdm1wLUA/fr1a/agknzcnWdmFfH20hJeWbCJUw/vriIgEieJ7jjlMuARd+8DnAk8bmafyuTu97v7eHcfX1hY+KkXkbYlEnF+/uIifvDsXD5au53TRnTnjotGqQiIxEk89wjWA31jpvsE82JdA0wCcPcPzCwbKACK45hLkpi785PJC3h82hq+/JkB3HzWCNLSVABE4qnJPQIz+6yZ5QT3rzCzO82sf4jXngEMNbOBZtYOuBSY3GCZtcApwWsfDmQDJQfyC0jbsaOqhp8+v5DHp63h2hMG8ZOzVQREWkKYQ0N/Anaa2Wjgu8AK4LGmnuTutcB1wCvAIqJnBy0ws1vN7Jxgse8CXzOzj4EngS+7ux/E7yGt3DMz13HsL17nkfdXc+XE/vzw84fpUJBICwlzaKjW3d3MzgXucfcHzeyaMC8eXBMwpcG8m2PuLwQ+eyCBpW2pizgPvbuK26Ys4jODu/J/Zx7OEb07JTqWSEoJUwjKzeyHwBXACUFjrrptlEM2beUWbvzHXFZv2cnpI7rzh8vHfmqcYBGJvzCHhi4BqoFr3H0T0UbfO+KaStq8Oeu285VHZpBmxn1XjOO+K45SERBJkCb3CIKN/50x02sJ0UYgsj/Li8v58sPTKcjN4qlrJ9KtY3aiI4mktDBnDV1gZsvMrMzMdphZuZntaIlw0vas27qTKx+cTmZ6Gn+95hgVAZEkEKaN4NfA2e6+KN5hpO1yd56euY6fv7AIDJ7+72Pp11VDQIokgzCFYLOKgByKTWVVfP/Zj3lnWSnHDMzn9gtHMaAgJ9GxRCSw30JgZhcEd2ea2d+BfxNtNAbA3f8Z52zSBrg733hiFks2lfOzc0fyxWP66yIxkSTT2B7B2TH3dwKnx0w7oEIgTZoybxMfrd3Ory8cxX8d3bfpJ4hIi9tvIXD3q1syiLQ9m3dUcfvLizmsRx4XHtUn0XFEZD/CnDX0qJl1jpnuYmYPxTeWtHZPTl/L534zlU1lVfzk7JGk63CQSNIK01g8yt2310+4+zYzGxvHTNKKuTt/nLqCO15ZwvFDC/j5eUfQv6sahkWSWZhCkGZmXdx9G4CZ5Yd8nqSY5+as567XlrGqtJJzx/TiNxePJjM90UNeiEhTwmzQfwt8YGbPAAZcBNwW11TS6jzw9kpum7KII3t34neXjOac0b11OEiklQjTxcRjZjYTOJno2UIXNBiAXlLcs7OKuG3KIr5wZE9+d8kY2mVoL0CkNQl7iCeT6N5A/X0RAJYXV/Djf89n4qB87r50DBk6FCTS6oQ5a+gG4AmiQ0h2A/4aDDYvKa6mLsINT31E+3bp3H3pWBUBkVYqzB7BNcAx7l4JYGa3Ax8Af4hnMEl+97+9kgUbdnDfFUfRXZ3HibRaYb7CGVAXM13HJ4eJJEWtLKng7teXceaRPZh0RI9ExxGRQxBmj+Bh4EMz+xfRAnAu8GBcU0lSc3d+MnkBWRlp3HLOyETHEZFDFOasoTvNbCpwHNGzhq5294/iHUyS1ysLNvPOslJ+cvYIuuXpkJBIa3cgrXvW4KekoLKdNdz6/AKGd8/jyon9Ex1HRJpBmLOGbgYeBboQPXPoYTO7Kd7BJPm4O//373kUl1dz+0WjdJaQSBsRpo3gi8Bod68CMLNfAXOAn8czmCSfv05bw4tzN/L9M4Yzpm/npp8gIq1CmK90G4DYA8FZwPr4xJFk9fqizfxk8gJOPqwbXz9xcKLjiEgzCrNHUAYsMLP/EG0sPg2Ybma/B3D36+OYT5JA2a4avvX3OYzs1Yk/XDZWfQiJtDFhCsG/glu9qfGJIsnq0fdXU15Vy68uPJKcLHU8K9LWhDl99FEzaw/0c/clLZBJkkhFdS0PvbeKUw/vxshenRIdR0TiIMxZQ2cTbRx+OZgeY2aT4x1MksOTH65l+84arjt5aKKjiEichGksvgWYAGwHcPc5wKA4ZpIkUVMX4aH3VnHsoK46S0ikDQtTCGrcvazBvEg8wkhymTJvIxvLqvjq8QMTHUVE4ihMy98CM7scSDezocD1wPvxjSWJVhdxHnhnJYMKcvjc8G6JjiMicRRmj+CbwEigGvgb0dNJvxXPUJJ4d7+2lPnrd3DdyUNI0+miIm1amLOGdgI/Cm6SAt5aWsLv31jORUf14fyxvRMdR0TiTJ3FyF4iEecXLy5iUEEOPz/vCMy0NyDS1qkQyF5eXrCJJZvLueHUoWRnpic6joi0gEYLgZmlm9m3WyqMJFYk4tz92jIGF+Zw1qheiY4jIi2k0ULg7nXAZQf74mY2ycyWmNlyM7txP8v8l5ktNLMFZva3g30vOXT1ewPXnzJU/QmJpJAwp4++Z2b3AH8HKutnuvvsxp5kZunAvUQ7qSsCZpjZZHdfGLPMUOCHwGfdfZuZ6TzFBNHegEjqClMIxgQ/b42Z58DJTTxvArDc3VcCmNlTRMc7XhizzNeAe919G4C7F4cJLc3vpfnRvYG7Lx2jvQGRFBPm9NHPHeRr9wbWxUwXAcc0WGYYgJm9B6QDt7j7yw1fyMyuBa4F6Nev30HGkf2JRJy7X1/KkG652hsQSUFhOp3rbmYPmtlLwfQIM7ummd4/AxgKnES0LeIBM/tUpzbufr+7j3f38YWFhc301lLvpfmbWLq5Qm0DIikqzOmjjwCvAPVfFZcS7sri9UDfmOk+fHpksyJgsrvXuPuq4LXVzWULqovZG/jCkT0THUdEEiBMIShw96cJOppz91qgLsTzZgBDzWygmbUDLgUadl/9b6J7A5hZAdFDRSvDRZfm8OT0tSzdXMG3Tx2mvQGRFBWmEFSaWVeiDcSY2USi/Q01KigY1xHdm1gEPO3uC8zsVjM7J1jsFWCLmS0E3gS+7+5bDuL3kIOwpaKaO15ZwrGDunLmkT0SHUdEEiTMWUPfIfpNfnDQqFsIXBTmxd19CjClwbybY+578PrfCRtYms/vXltKZXUtt547Ul1JiKSwMGcNzTazE4HhgAFL3L0m7skkrkorqnl6ZhEXj+/D0O55iY4jIgnUZCEws2zgf4DjiB4eesfM7nP3qniHk/h57IM11NRF+OrxGmxOJNWFOTT0GFAO/CGYvhx4HLg4XqEkvnbtruPxD1Zz6uHdGVyYm+g4IpJgYQrBEe4+Imb6zaBxV1qpZ2etY9vOGq49QXsDIhLurKHZwZlCAJjZMcDM+EWSeKqLOH95dxVj+nZmfP8uiY4jIkkgzB7BUcD7ZrY2mO4HLDGzeURP/BkVt3TS7F5dsIk1W3by/yYdpjOFRAQIVwgmxT2FtIhIxLnnzeX0y+/AGSN13YCIRIU5fXRNSwSR+Jv88QYWbNjBXZeoh1ER+YSGqkwR1bV1/ObVJYzs1ZFzRquHURH5hApBirhv6kqKtu3ih58/nDTtDYhIjDDdUH/TzHR6SSu2vLiCe99czjmje3Hc0IJExxGRJBNmj6A70WEmnw7GINbXyVbmthcX0r5dOj8+a0TTC4tIymmyELj7TUTHCHgQ+DKwzMx+YWaD45xNmsGmsiqmLi3hqmP7U5iXleg4IpKEQrURBL2EbgputUAX4Fkz+3Ucs0kz+Pec9bjDBeP6JDqKiCSpMJ3O3QB8CSgF/kJ0zIAaM0sDlgE/iG9EOVjuzj9mFXFU/y4MKMhJdBwRSVJhLijLBy5oeD2Bu0fM7Kz4xJLmMGfddpYVV3Db+UckOoqIJLEwh4ZeArbWT5hZx6C/Idx9UbyCyaFxd25/eTH5Oe103YCINCpMIfgTUBEzXRHMkyT2+qJipq3cyrdOHUpedmai44hIEgtTCCxoLAaih4QId0hJEsTdueOVJQwqyOGyCf0SHUdEklyYQrDSzK43s8zgdgOwMt7B5OC9vayUJZvL+d/PDSEzXRePi0jjwmwlvg58BlgPFAHHANfGM5Qcmr+8s5JueVmcrbYBEQkhTO+jxcClLZBFmsGSTeW8s6yU758xnHYZ2hsQkaaFHbz+GmAkkF0/392/EsdccpCembmOzHTjcrUNiEhIYb4yPg70AM4A3gL6EB3MXpJMXcR5fu4GThzWjS457RIdR0RaiTCFYIi7/xiodPdHgS8QbSeQJPPhyi1s3lHNeWPVNiAi4YUpBDXBz+1mdgTQCegWv0hysJ6bs4Gcdumcclj3REcRkVYkzPUA9wfjEdwETAZygR/HNZUcsDVbKnnu4/WcPaoX7dulJzqOiLQijRaCoGO5He6+DXgbGNQiqeSAuDs3/Xs+GWlpfOf0YYmOIyKtTKOHhoKriNW7aJJ7cd5G3llWyg8mDadnp/aJjiMirUyYNoLXzOx7ZtbXzPLrb3FPJqHURZzf/Wcpw7vn8cVj+ic6joi0QmHaCC4Jfv5vzDxHh4mSwvMfb2BFSSV//OI40jUovYgchDBXFg9siSBy4NydP7yxjMN65DFpZI9ExxGRVirMlcVf2td8d3+s+ePIgZhbVMaKkkpuv/BI0rQ3ICIHKcyhoaNj7mcDpwCzARWCBHth7gYy041JI3smOoqItGJhDg19M3bazDoDT8UtkYQSiTgvzt3I8UML6dRBA8+IyME7mO4pKwG1GyTYR+u2saGsirNGaW9ARA5Nk4XAzJ43s8nB7QVgCfCvMC9uZpPMbImZLTezGxtZ7kIzczMbHz56ant2VhHZmWmcNkLdSYjIoQnTRvCbmPu1wBp3L2rqSWaWDtwLnEZ0QJsZZjbZ3Rc2WC4PuAH4MHTqFFe2s4Z/fbSe88f21njEInLIwhwaWgt86O5vuft7wBYzGxDieROA5e6+0t13E21XOHcfy/0MuB2oChdZnpm1jqqaCFdOHJDoKCLSBoQpBM8AkZjpumBeU3oD62Kmi4J5e5jZOKCvu7/Y2AuZ2bVmNtPMZpaUlIR467YrEnEe+2ANEwbkM6JXx0THEZE2IEwhyAi+0QMQ3D/kUU+CDu3uBL7b1LLufr+7j3f38YWFhYf61q3aW0tLWLt1J1/6jLqTEJHmEaYQlJjZOfUTZnYuUBrieeuBvjHTfYJ59fKAI4CpZrYamAhMVoNx4x79YDXd8rI4Q1cSi0gzCdNY/HXgCTO7J5guAvZ5tXEDM4ChZjaQaAG4FLi8/kF3LwMK6qfNbCrwPXefGS566lldWsnUJSV8+9RhZKZrYHoRaR5hLihbAUw0s9xguiLMC7t7rZldB7wCpAMPufsCM7sVmOnukw8hd8pxd25/eTGZ6cZlE/o2/QQRkZDC9DX0C+DX7r49mO4CfNfdb2rque4+BZjSYN7N+1n2pDCBU9VjH6zhpfmb+H+TDqNbx+xExxGRNiTM8YXP1xcBgGC0sjPjF0kaWlFSwW0vLuLkw7rx3yeo928RaV5hCkG6mWXVT5hZeyCrkeWlGbk7P3luAVmZadx+4Sj1MioizS5MY/ETwOtm9nAwfTXwaPwiSayX5m/i3eWl3HruSArzVH9FpPmFaSy+3czmEu1+GuBn7v5KfGNJvYffW8WgghwNQykicRNmjwB3fwl4Kc5ZpIGibTuZsXob3zt9mIahFJG4CdP76EQzm2FmFWa228zqzGxHS4RLdZM/3gDAuWN6N7GkiMjBC9NYfA9wGbAMaA98lWivohJnk+dsYFy/zvTN75DoKCLShoW6PNXdlwPp7l7n7g8Dk+IbS95bXsriTeWcP1Z7AyISX2HaCHaaWTtgjpn9GtjIwY1sJiHV1kX46fML6JffgYvH6ypiEYmvMBv0K4PlriM6TGVf4MJ4hkp1j36whqWbK/jRFw4nOzM90XFEpI0Lc/romuBuFfDT+MaRt5aW8IspizjlsG6crmEoRaQF6BBPEtmwfRf/89dZDOuex12XjsFMp4yKSPypECSRR95fTVVthD9fcZTGIhaRFqNCkCQqqmt58sO1fP6IHvTrqtNFRaTlhOmG+nnAG8wuA2YCf3Z3DTp/iNyd+99eSXl1LV89Xr2LikjLCnP66EqgEHgymL4EKAeGAQ8QPatIDtKWimqufmQGc4vKOGl4IWP6dk50JBFJMWEKwWfc/eiY6efNbIa7H21mC+IVLFX8ceoK5q8v446LRuniMRFJiDBtBLlm1q9+IrifG0zujkuqFFG8o4q/TlvD+WP7cPH4vmRoHGIRSYAwewTfBd41sxWAAQOB/zGzHDQuwUErLq/i5ucWUBtxrj9lSKLjiEgKC3NB2RQzGwocFsxaEtNAfFfckrVhs9du4/IHprG7NsI3Tx5K/645iY4kIiks1HgEwFHAgGD50WaGuz8Wt1RtWCTi3DJ5AZ3bt+OpaycyoEBFQEQSK8zpo48Dg4E5QF0w2wEVggPk7jw1Yx1zi8q4879GqwiISFIIs0cwHhjh7g2vJZADsLy4nK88MpO1W3cyum9nztNgMyKSJMIUgvlAD6LdT8tB+ss7qyitqOaXFxzJmUf2JE1DT4pIkghTCAqAhWY2Haiun+nu58QtVRuzc3ctL8zdyJlH9uSyCf2afoKISAsKUwhuiXeItu7l+ZuoqK7l4qP6JDqKiMinhDl99K2WCNJWuTtPTl9Lv/wOTBiYn+g4IiKfst9LWc3s3eBnuZntiLmVm9mOlovYuj05fR0zVm/ja8cP1PgCIpKU9rtH4O7HBT/zWi5O27JmSyU/e2Ehxw0p4IvH9E90HBGRfQp1QZmZpQPdY5d397XxCtVW3PvmchznjotH6SwhEUlaYS4o+ybwE2AzEAlmOzAqjrlavdKKav49ZwMXH9WHnp3aJzqOiMh+hdkjuAEY7u5b4h2mLXli2lp210a4+rMDEx1FRKRRYfo9Xkd0RDIJaVNZFY+8v4qThhcypFtu008QEUmgsCOUTTWzF9n7grI745aqFdtdG+F//zab6toIN33h8ETHERFpUphCsDa4tQtush/vLCvhthcXsXhTOfdcPpYh3XTClYgkvzAXlP20JYK0diXl1Vz98Ax6d2nPPZeP5axRvRIdSUQklP0WAjO7y92/ZWbPEz1LaC9h+hoys0nA3UA68Bd3/1WDx78DfBWoBUqAr7j7mgP7FZLD5I83UBtxHrxqvPYERKRVaWyP4PHg528O5oWDaw/uBU4DioAZZjbZ3RfGLPYRMN7dd5rZN4BfA5cczPsl2j9mFTG6TycVARFpdRq7snhW8PNg+xqaACx395UAZvYUcC6wpxC4+5sxy08DrjjI90qISMS56/VlRCLOwo07+Ok5IxMdSUTkgIW5oGwo8EtgBJBdP9/dBzXx1N5ETz2tVwQc08jy1wAv7SfDtcC1AP36JU83zm8tK+H3ry8DoF16GmePVruAiLQ+Yc4aepjolcW/Az4HXE246w9CM7MriI6EduK+Hnf3+4H7AcaPH580I6U98t5qCvOyeOraidTWOfk5OqlKRFqfMIWgvbu/bmYWNOTeYobva4IAAA10SURBVGazgJubeN56oG/MdJ9g3l7M7FTgR8CJ7l7d8PFks6Oqhq8+MpPB3XJ5a2kJ3z51GIMLddGYiLReYQpBtZmlAcvM7DqiG/MwW74ZwFAzGxg851Lg8tgFzGws8GdgkrsXH1DyBPnX7PVMX72VGWu20i49jcuPSZ5DVSIiByNsX0MdgOuBnxE9PHRVU09y99qgcLxC9PTRh9x9gZndCsx098nAHUSLyjNBX/1rk3kITHfniQ/XMLpPJ+68ZAw7dtVQmJeV6FgiIoek0UIQnAJ6ibt/D6gg2j4QmrtPAaY0mHdzzP1TD+T1Em3G6m0s3VzBry8cpcNBItJmNDZCWYa71wHHtWCepPbEh2vIy8rgrNE9Ex1FRKTZNLZHMB0YB3xkZpOBZ4DK+gfd/Z9xzpZUtlRU89K8TVx+TD86tAs1no+ISKsQZouWDWwBTiba1YQFP1OqEDw7q4jddRE1DotIm9NYIegW9AU0n08KQL2kOZe/JUQizt+mr2XCgHyGdVcXEiLStjRWCNKJntGzr8F2U6oQvLeilDVbdvKd04YlOoqISLNrrBBsdPdbWyxJEnti2lryc9ox6YgeiY4iItLsGusqYl97Ailn844q/rNoMxcf1YesjPRExxERaXaN7RGc0mIpklB1bR1/mrqCmau3URdxLpugRmIRaZsa64Z6a0sGSTavLyrmrteW0TE7g4uP6sOAgpxERxIRiQudEL8f7ywrJS8rg9k/Po2M9GbtbFVEJKloC7cf7y4vYeLgrioCItLmaSu3D2u2VLJu6y6OH1qQ6CgiInGnQrAP7ywrBeC4ISoEItL2qRDsw7vLSunduT0D1UAsIilAhaCB3bUR3l1eygnDCgjGSBARadNUCBqYuXorFdW1fG54t0RHERFpESoEDbyxuJh26Wl8Vu0DIpIiVAgaeGNJMccMyicnS5dYiEhqUCGIsbq0kpUllZx8mA4LiUjqUCGI8cA7K8lIM04fqV5GRSR1qBAEVpVW8tSMdVx+TD96d26f6DgiIi0m5Q6Eb9+5m/bt0snKSKe4vIr123ZRXF7NH6euICsjjW+ePDTREUVEWlRKFQJ35wu/f5eszDQun9CP3766lF01dQDk57Tjp+eMpDAvK8EpRURaVkoVgpLyatZv3wXAz19cxIQB+Xz9pEF0aJfBUf27kKkO5kQkBaVUIVhWXAHA7y4ZTSQC54zppY2/iKS8lCoEy4NC8NnBBXTrmJ3gNCIiySGlvg4vKy6nY3aG2gFERGKkViHYXMGQbrnqTE5EJEZKFYIVJRUM7ZaX6BgiIkklZQrB1srdlFbsZmj33ERHERFJKilTCOobigd3UyEQEYmVcoVgqAqBiMheUqYQFOS247QR3enVSf0IiYjESpnrCE4f2UO9ioqI7EPK7BGIiMi+xbUQmNkkM1tiZsvN7MZ9PJ5lZn8PHv/QzAbEM4+IiHxa3AqBmaUD9wKfB0YAl5nZiAaLXQNsc/chwO+A2+OVR0RE9i2eewQTgOXuvtLddwNPAec2WOZc4NHg/rPAKabLfkVEWlQ8C0FvYF3MdFEwb5/LuHstUAZ0bfhCZnatmc00s5klJSVxiisikppaRWOxu9/v7uPdfXxhYWGi44iItCnxLATrgb4x032CeftcxswygE7AljhmEhGRBuJZCGYAQ81soJm1Ay4FJjdYZjJwVXD/IuANd/c4ZhIRkQYsnttdMzsTuAtIBx5y99vM7FZgprtPNrNs4HFgLLAVuNTdVzbxmiXAmoOMVACUHuRz4y1ZsynXgVGuA5es2dparv7uvs9j63EtBMnGzGa6+/hE59iXZM2mXAdGuQ5csmZLpVytorFYRETiR4VARCTFpVohuD/RARqRrNmU68Ao14FL1mwpkyul2ghEROTTUm2PQEREGlAhEBFJcSlTCJrqErsFc/Q1szfNbKGZLTCzG4L5t5jZejObE9zOTEC21WY2L3j/mcG8fDP7j5ktC352aeFMw2PWyRwz22Fm30rU+jKzh8ys2Mzmx8zb5zqyqN8Hn7m5ZjauhXPdYWaLg/f+l5l1DuYPMLNdMevuvhbOtd+/nZn9MFhfS8zsjHjlaiTb32NyrTazOcH8FllnjWwf4vsZc/c2fyN6QdsKYBDQDvgYGJGgLD2BccH9PGAp0W66bwG+l+D1tBooaDDv18CNwf0bgdsT/HfcBPRP1PoCTgDGAfObWkfAmcBLgAETgQ9bONfpQEZw//aYXANil0vA+trn3y74P/gYyAIGBv+z6S2ZrcHjvwVubsl11sj2Ia6fsVTZIwjTJXaLcPeN7j47uF8OLOLTvbImk9iuwh8FzktgllOAFe5+sFeWHzJ3f5voVfCx9reOzgUe86hpQGcz69lSudz9VY/26gswjWh/Xy1qP+trf84FnnL3andfBSwn+r/b4tnMzID/Ap6M1/vvJ9P+tg9x/YylSiEI0yV2i7PoiGxjgQ+DWdcFu3cPtfQhmIADr5rZLDO7NpjX3d03Bvc3Ad0TkKvepez9j5no9VVvf+somT53XyH6zbHeQDP7yMzeMrPjE5BnX3+7ZFpfxwOb3X1ZzLwWXWcNtg9x/YylSiFIOmaWC/wD+Ja77wD+BAwGxgAbie6WtrTj3H0c0VHl/tfMToh90KP7ogk539iiHReeAzwTzEqG9fUpiVxH+2NmPwJqgSeCWRuBfu4+FvgO8Dcz69iCkZLyb9fAZez9paNF19k+tg97xOMzliqFIEyX2C3GzDKJ/pGfcPd/Arj7Znevc/cI8ABx3CXeH3dfH/wsBv4VZNhcv6sZ/Cxu6VyBzwOz3X1zkDHh6yvG/tZRwj93ZvZl4Czgi8EGhODQy5bg/iyix+KHtVSmRv52CV9fsKdL/AuAv9fPa8l1tq/tA3H+jKVKIQjTJXaLCI49Pggscvc7Y+bHHtc7H5jf8LlxzpVjZnn194k2NM5n767CrwKea8lcMfb6hpbo9dXA/tbRZOBLwZkdE4GymN37uDOzScAPgHPcfWfM/EKLjimOmQ0ChgKN9vrbzLn297ebDFxqZllmNjDINb2lcsU4FVjs7kX1M1pqne1v+0C8P2PxbgVPlhvR1vWlRCv5jxKY4ziiu3VzgTnB7Uyi3XHPC+ZPBnq2cK5BRM/Y+BhYUL+OiA4d+jqwDHgNyE/AOsshOmBRp5h5CVlfRIvRRqCG6PHYa/a3joieyXFv8JmbB4xv4VzLiR4/rv+c3Rcse2HwN54DzAbObuFc+/3bAT8K1tcS4PMt/bcM5j8CfL3Bsi2yzhrZPsT1M6YuJkREUlyqHBoSEZH9UCEQEUlxKgQiIilOhUBEJMWpEIiIpDgVAklaZuZm9tuY6e+Z2S3N9NqPmNlFzfFaTbzPxWa2yMzebDC/YW+Wc8zsS834vieZ2QvN9XrStmUkOoBII6qBC8zsl+5emugw9cwswz/pzK0p1wBfc/d39/HYCncf04zRRA6K9ggkmdUSHZ/12w0faPiN3swqgp8nBZ2CPWdmK83sV2b2RTObbtGxFgbHvMypZjbTzJaa2VnB89Mt2o//jKBTtP+Oed13zGwysHAfeS4LXn++md0ezLuZ6AVCD5rZHWF/aTOrMLPfWbQ/+tfNrDCYP8bMptkn4wvU90k/xMxeM7OPzWx2zO+Ya2bPWnRMgieCq1YJ1snC4HV+EzaXtGHxvHJPN90O5QZUAB2JjpPQCfgecEvw2CPARbHLBj9PArYT7dc9i2i/Kz8NHrsBuCvm+S8T/TI0lOiVpdnAtcBNwTJZwEyifeOfBFQCA/eRsxewFigkupf9BnBe8NhU9nG1J9H+7XfxydWjc4Djg8ecaN9AADcD9wT35wInBvdvjfldPgTOD+5nAx2CvGVE+55JAz4gWpS6Er1qt/5i0s6J/jvrlvib9ggkqXm058XHgOsP4GkzPNqvezXRS+9fDebPI7oBrve0u0c82tXwSuAwon0sfcmiI1N9SHTDOTRYfrpH+8lv6GhgqruXePSQ0RNEBz1pygp3HxNzeyeYH+GTDs/+ChxnZp2IbrTfCuY/CpwQ9A/V293/BeDuVf5Jv0LT3b3Io527zQl+9zKgiuheygXAnj6IJHWpEEhrcBfRY+05MfNqCT6/ZpZGdOS5etUx9yMx0xH2bhdr2L+KE+275ZsxG+eB7l5fSCoP6bc4eAfbD0zseqgjOlpZLdHePp8l2ivpy4eYTdoAFQJJeu6+FXiaaDGotxo4Krh/DpB5EC99sZmlBcfUBxE9ZPIK8I2gK2DMbFjQG2tjpgMnmllB0EPlZcBbTTynMWlAffvH5cC77l4GbLNPBkS5EnjLo6NYFZnZeUHeLDPrsL8Xtmg/953cfQrRtpfRh5BT2gidNSStxW+B62KmHwCeM7OPiX6rPZhv62uJbsQ7Eu1tssrM/kL0EMrsoHG1hCaG53T3jWZ2I/Am0T2KF909THfdg4NDUPUecvffE/1dJpjZTUT7nb8kePwq4L5gQ78SuDqYfyXwZzO7lWhPmhc38p55RNdbdpD1OyFyShun3kdFkoyZVbh7bqJzSOrQoSERkRSnPQIRkRSnPQIRkRSnQiAikuJUCEREUpwKgYhIilMhEBFJcf8fsj4l6sy8+rkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAfp0XnrL56S",
        "outputId": "7ec0a0b7-52a4-44b3-ab6f-594fa86df926"
      },
      "source": [
        "seed_text = \"some say\"\n",
        "next_words = 12\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "some say that he is not allowed by law within a hundred yards of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rJtD5YYNg_x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}